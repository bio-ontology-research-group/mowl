{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# ELBoxEmbeddings\n\nThis example is based on the paper `Description Logic EL++ Embeddings with Intersectional Closure <https://arxiv.org/abs/2202.14018v1>`_. This paper is based on the idea of :doc:`/examples/elmodels/plot_1_elembeddings`, but in this work the main point is to solve the *intersectional closure* problem.\n\nIn the case of :doc:`/examples/elmodels/plot_1_elembeddings`, the geometric objects representing ontology classes are $n$-dimensional balls. One of the normal forms in EL is:\n\n\\begin{align}C_1 \\sqcap C_2 \\sqsubseteq D\\end{align}\n\nAs we can see, there is an intersection operation $C_1 \\sqcap C_2$. Computing this intersection using balls is not a closed operations because the region contained in the intersection of two balls is not a ball. To solve that issue, this paper proposes the idea of changing the geometric objects to boxes, for which the intersection operation has the closure property.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example is quite similar to the one found in :doc:`/examples/elmodels/plot_1_elembeddings`.\nThere might be slight changes in the training part but the most important changes are in the\n`Definition of loss functions`_ definition of the loss functions for each normal form.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mowl.base_models.elmodel import EmbeddingELModel\nimport mowl.models.elboxembeddings.losses as L\nfrom mowl.nn.elmodule import ELModule\nimport math\nimport logging\nimport numpy as np\n\nfrom mowl.models.elboxembeddings.evaluate import ELBoxEmbeddingsPPIEvaluator\n\nfrom tqdm import trange, tqdm\n\nimport torch as th\nfrom torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definition of loss functions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ELBoxModule(ELModule):\n\n    def __init__(self, nb_ont_classes, nb_rels, embed_dim=50, margin=0.1):\n        super().__init__()\n        self.nb_ont_classes = nb_ont_classes\n        self.nb_rels = nb_rels\n\n        self.embed_dim = embed_dim\n\n        self.class_embed = nn.Embedding(self.nb_ont_classes, embed_dim)\n        nn.init.uniform_(self.class_embed.weight, a=-1, b=1)\n\n        weight_data = th.linalg.norm(self.class_embed.weight.data, axis=1).reshape(-1, 1)\n        self.class_embed.weight.data /= weight_data\n\n        self.class_offset = nn.Embedding(self.nb_ont_classes, embed_dim)\n        nn.init.uniform_(self.class_offset.weight, a=-1, b=1)\n        weight_data = th.linalg.norm(self.class_offset.weight.data, axis=1).reshape(-1, 1)\n        self.class_offset.weight.data /= weight_data\n\n        self.rel_embed = nn.Embedding(nb_rels, embed_dim)\n        nn.init.uniform_(self.rel_embed.weight, a=-1, b=1)\n        weight_data = th.linalg.norm(self.rel_embed.weight.data, axis=1).reshape(-1, 1)\n        self.rel_embed.weight.data /= weight_data\n\n        self.margin = margin\n\n    def gci0_loss(self, data, neg=False):\n        c = self.class_embed(data[:, 0])\n        d = self.class_embed(data[:, 1])\n\n        off_c = th.abs(self.class_offset(data[:, 0]))\n        off_d = th.abs(self.class_offset(data[:, 1]))\n\n        euc = th.abs(c - d)\n        dst = th.reshape(th.linalg.norm(th.relu(euc + off_c - off_d + self.margin), axis=1),\n                         [-1, 1])\n\n        return dst\n\n    def gci1_loss(self, data, neg=False):\n        c = self.class_embed(data[:, 0])\n        d = self.class_embed(data[:, 1])\n        e = self.class_embed(data[:, 2])\n        off_c = th.abs(self.class_offset(data[:, 0]))\n        off_d = th.abs(self.class_offset(data[:, 1]))\n        off_e = th.abs(self.class_offset(data[:, 2]))\n\n        startAll = th.maximum(c - off_c, d - off_d)\n        endAll = th.minimum(c + off_c, d + off_d)\n\n        new_offset = th.abs(startAll - endAll) / 2\n\n        cen1 = (startAll + endAll) / 2\n        euc = th.abs(cen1 - e)\n\n        dst = th.reshape(th.linalg.norm(th.relu(euc + new_offset - off_e + self.margin), axis=1),\n                         [-1, 1]) + th.linalg.norm(th.relu(startAll - endAll), axis=1)\n        return dst\n\n    def gci1_bot_loss(self, data, neg=False):\n        c = self.class_embed(data[:, 0])\n        d = self.class_embed(data[:, 1])\n\n        off_c = th.abs(self.class_offset(data[:, 0]))\n        off_d = th.abs(self.class_offset(data[:, 1]))\n\n        euc = th.abs(c - d)\n        dst = th.reshape(th.linalg.norm(th.relu(-euc + off_c + off_d + self.margin), axis=1),\n                         [-1, 1])\n        return dst\n\n    def gci2_loss(self, data, neg=False):\n        if neg:\n            return self.gci2_loss_neg(data)\n        else:\n            c = self.class_embed(data[:, 0])\n            r = self.rel_embed(data[:, 1])\n            d = self.class_embed(data[:, 2])\n\n            off_c = th.abs(self.class_offset(data[:, 0]))\n            off_d = th.abs(self.class_offset(data[:, 2]))\n\n            euc = th.abs(c + r - d)\n            dst = th.reshape(th.linalg.norm(th.relu(euc + off_c - off_d + self.margin), axis=1),\n                             [-1, 1])\n            return dst\n\n    def gci2_loss_neg(self, data):\n        c = self.class_embed(data[:, 0])\n        r = self.rel_embed(data[:, 1])\n\n        rand_index = np.random.choice(self.class_embed.weight.shape[0], size=len(data))\n        rand_index = th.tensor(rand_index).to(self.class_embed.weight.device)\n        d = self.class_embed(rand_index)\n\n        off_c = th.abs(self.class_offset(data[:, 0]))\n        off_d = th.abs(self.class_offset(rand_index))\n\n        euc = th.abs(c + r - d)\n        dst = th.reshape(th.linalg.norm(th.relu(euc - off_c - off_d - self.margin), axis=1),\n                         [-1, 1])\n        return dst\n\n    def gci3_loss(self, data, neg=False):\n        r = self.rel_embed(data[:, 0])\n        c = self.class_embed(data[:, 1])\n        d = self.class_embed(data[:, 2])\n\n        off_c = th.abs(self.class_offset(data[:, 1]))\n        off_d = th.abs(self.class_offset(data[:, 2]))\n\n        euc = th.abs(c - r - d)\n        dst = th.reshape(th.linalg.norm(th.relu(euc - off_c - off_d + self.margin), axis=1),\n                         [-1, 1])\n        return dst\n\n\nclass ELBoxEmbeddings(EmbeddingELModel):\n\n    def __init__(self,\n                 dataset,\n                 embed_dim=50,\n                 margin=0,\n                 reg_norm=1,\n                 learning_rate=0.001,\n                 epochs=1000,\n                 batch_size=4096 * 8,\n                 model_filepath=None,\n                 device='cpu'\n                 ):\n        super().__init__(dataset, batch_size, extended=True, model_filepath=model_filepath)\n\n        self.embed_dim = embed_dim\n        self.margin = margin\n        self.reg_norm = reg_norm\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.device = device\n        self._loaded = False\n        self._loaded_eval = False\n        self.extended = False\n        self.init_model()\n\n    def init_model(self):\n        self.model = ELBoxModule(\n            len(self.class_index_dict),\n            len(self.object_property_index_dict),\n            embed_dim=self.embed_dim,\n            margin=self.margin\n        ).to(self.device)\n\n    def train(self):\n        criterion = nn.MSELoss()\n        optimizer = th.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n        best_loss = float('inf')\n\n        training_datasets = {k: v.data for k, v in\n                             self.training_datasets.items()}\n        validation_dataset = self.validation_datasets[\"gci2\"][:]\n\n        for epoch in trange(self.epochs):\n            self.model.train()\n\n            train_loss = 0\n            loss = 0\n            for gci_name, gci_dataset in training_datasets.items():\n                if len(gci_dataset) == 0:\n                    continue\n                rand_index = np.random.choice(len(gci_dataset), size=512)\n                dst = self.model(gci_dataset[rand_index], gci_name)\n                mse_loss = criterion(dst, th.zeros(dst.shape, requires_grad=False).to(self.device))\n                loss += mse_loss\n\n                if gci_name == \"gci2\":\n                    rand_index = np.random.choice(len(gci_dataset), size=512)\n                    gci_batch = gci_dataset[rand_index]\n                    prots = [self.class_index_dict[p] for p in self.dataset.evaluation_classes.as_str]\n                    idxs_for_negs = np.random.choice(prots, size=len(gci_batch), replace=True)\n                    rand_prot_ids = th.tensor(idxs_for_negs).to(self.device)\n                    neg_data = th.cat([gci_batch[:, :2], rand_prot_ids.unsqueeze(1)], dim=1)\n\n                    dst = self.model(neg_data, gci_name, neg=True)\n                    mse_loss = criterion(dst,\n                                         th.ones(dst.shape, requires_grad=False).to(self.device))\n                    loss += mse_loss\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.detach().item()\n\n            with th.no_grad():\n                self.model.eval()\n                valid_loss = 0\n                gci2_data = validation_dataset\n                dst = self.model(gci2_data, \"gci2\")\n                loss = criterion(dst, th.zeros(dst.shape, requires_grad=False).to(self.device))\n                valid_loss += loss.detach().item()\n\n            checkpoint = 1\n            if best_loss > valid_loss:\n                best_loss = valid_loss\n                th.save(self.model.state_dict(), self.model_filepath)\n            if (epoch + 1) % checkpoint == 0:\n                print(f'\\nEpoch {epoch}: Train loss: {train_loss:.4f} Valid loss: {valid_loss:.4f}')\n\n    def evaluate_ppi(self):\n        self.init_model()\n        print('Load the best model', self.model_filepath)\n        self.model.load_state_dict(th.load(self.model_filepath))\n        with th.no_grad():\n            self.model.eval()\n\n            eval_method = self.model.gci2_loss\n\n            evaluator = ELBoxEmbeddingsPPIEvaluator(\n                self.dataset.testing, eval_method, self.dataset.ontology,\n                self.class_index_dict, self.object_property_index_dict, device=self.device)\n            evaluator()\n            evaluator.print_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mowl.datasets.builtin import PPIYeastSlimDataset\n\ndataset = PPIYeastSlimDataset()\n\nmodel = ELBoxEmbeddings(dataset,\n                     embed_dim=10,\n                     margin=0.1,\n                     reg_norm=1,\n                     learning_rate=0.001,\n                     epochs=20,\n                     batch_size=4096,\n                     model_filepath=None,\n                     device='cpu')\n\nmodel.train()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}